{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from MarkovDecisionProcess import MarkovDecisionProcess\n",
    "from PolicyNetwork import PolicyNetwork\n",
    "from ValueNetwork import ValueNetwork\n",
    "from ActorCritic import ActorCritic\n",
    "from solution import solve \n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**States:** Wealth and income shock, $S_t = (W_{t-1}, Z_t)$\n",
    "\n",
    "**Action:** Consumption, $A_t = C_t$\n",
    "\n",
    "**Transitions:**\n",
    "\n",
    "\\begin{align*}\n",
    "    W_t &= W_{t-1}R + Z_t - C_t \\\\\n",
    "    Z_t &= \\rho Z_{t-1} + \\epsilon_t \\\\\n",
    "    \\epsilon_t &\\sim \\log N(0, \\sigma_\\epsilon^2) \\\\\n",
    "    \\sigma_{\\epsilon} &= 0.25 \\\\\n",
    "    \\rho &= 0.5 \\\\\n",
    "    R &= 1.1 \\\\\n",
    "    W_{-1} &= 1\n",
    "\\end{align*}\n",
    "\n",
    "**Action space:** \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{A}(\\mathcal{S}) = \\{C_t \\geq 0 : W_{t-1} \\geq W_{min} = 0\\}\n",
    "\\end{align*}\n",
    "\n",
    "**Reward:**\n",
    "\n",
    "\\begin{align*}\n",
    "    U_t(C_t) &= \\frac{C_t^{1-\\sigma} - 1}{1 - \\sigma} \\\\\n",
    "    \\sigma &= 0.9\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_bounds_func(state):\n",
    "    R = 1.1\n",
    "    low = 0\n",
    "    high = state[0] * R + state[1]\n",
    "\n",
    "    return low, high\n",
    "\n",
    "def transition_func(state, action):\n",
    "    R = 1.1\n",
    "    sd = 0.1\n",
    "    mean = -0.5 * sd\n",
    "    rho = 0.25\n",
    "\n",
    "    epsilon = np.random.lognormal(mean, sd)\n",
    "    \n",
    "    new_state = state.copy()\n",
    "    new_state[0] = state[0] * R + state[1] - action[0]\n",
    "    new_state[1] = state[1] * rho + epsilon\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "def reward_func(state, action):\n",
    "    sigma = 0.9\n",
    "\n",
    "    return (action[0]**(1-sigma) - 1) / (1 - sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = MarkovDecisionProcess(state = np.array([10,0], dtype = float),\n",
    "                                action = np.array([0], dtype = float),\n",
    "                                action_bounds_func = action_bounds_func,\n",
    "                                transition_func = transition_func,\n",
    "                                reward_func = reward_func,\n",
    "                                T = 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [10.  0.] \n",
      "Current action: [0.] \n",
      "Current reward: None \n",
      "Transition function: transition_func(state, action) \n",
      "Reward function: reward_func(state, action)\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_network = ValueNetwork(example)\n",
    "policy_network = PolicyNetwork(example)\n",
    "actor_critic = ActorCritic(mdp = example,\n",
    "                           value_network = value_network,\n",
    "                           policy_network = policy_network,\n",
    "                           alpha_value = 0.01, alpha_policy = 0.01, alpha_reward = 0.01,\n",
    "                           lambda_value = 0.2, lambda_policy = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal(loc: tensor([0.], grad_fn=<AddBackward0>), scale: tensor([1.], grad_fn=<ExpBackward0>))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.tensor(example.get_state(), dtype=torch.float32)\n",
    "policy_network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward at t = 10000:  -0.031753151297569275\n",
      "State:  [2.17408323 1.24762698]\n",
      "Average reward at t = 20000:  -0.03846532571315765\n",
      "State:  [1.77497768 1.23627714]\n",
      "Average reward at t = 30000:  -0.06031827974319458\n",
      "State:  [1.88844097 1.15725276]\n",
      "Average reward at t = 40000:  -0.047780316472053525\n",
      "State:  [2.5510633  1.51888097]\n",
      "Average reward at t = 50000:  -0.04005705010890961\n",
      "State:  [3.16992021 1.22337559]\n",
      "Average reward at t = 60000:  -0.035222853183746336\n",
      "State:  [1.78068233 1.25494501]\n",
      "Average reward at t = 70000:  -0.023138280630111693\n",
      "State:  [0.72403753 1.26429424]\n",
      "Average reward at t = 80000:  -0.0724737604856491\n",
      "State:  [0.51724124 1.25097717]\n",
      "Average reward at t = 90000:  -0.018690505623817445\n",
      "State:  [3.79608965 1.06374379]\n",
      "Average reward at t = 100000:  -0.053693284153938294\n",
      "State:  [0.08629894 1.27730246]\n",
      "Average reward at t = 110000:  -0.05367028284072876\n",
      "State:  [1.59760964 1.1534783 ]\n",
      "Average reward at t = 120000:  -0.06267191481590272\n",
      "State:  [3.75892615 1.38289058]\n",
      "Average reward at t = 130000:  -0.051210833430290224\n",
      "State:  [3.92113018 1.27670766]\n",
      "Average reward at t = 140000:  -0.08528931999206543\n",
      "State:  [1.62093067 1.21313601]\n",
      "Average reward at t = 150000:  -0.06619758570194244\n",
      "State:  [1.76174641 1.34846913]\n",
      "Average reward at t = 160000:  -0.03728206932544708\n",
      "State:  [5.44576168 1.33144409]\n",
      "Average reward at t = 170000:  -0.03557753503322601\n",
      "State:  [0.66817379 1.13586026]\n",
      "Average reward at t = 180000:  -0.057522462606430055\n",
      "State:  [0.60742283 1.2516235 ]\n",
      "Average reward at t = 190000:  -0.08572165060043335\n",
      "State:  [1.9969244  1.41498546]\n",
      "Average reward at t = 200000:  -0.05231441676616669\n",
      "State:  [0.89908826 1.16411452]\n",
      "Average reward at t = 210000:  -0.09242051541805267\n",
      "State:  [0.07553387 1.20543001]\n",
      "Average reward at t = 220000:  -0.08002500689029693\n",
      "State:  [1.52717721 1.09692772]\n",
      "Average reward at t = 230000:  -0.06360705196857452\n",
      "State:  [0.40671039 1.26496041]\n",
      "Average reward at t = 240000:  -0.06324710404872894\n",
      "State:  [1.31690919 1.30738763]\n",
      "Average reward at t = 250000:  -0.03779300427436828\n",
      "State:  [2.90612864 1.2873701 ]\n",
      "Average reward at t = 260000:  -0.04860457074642181\n",
      "State:  [0.17248857 1.27975651]\n",
      "Average reward at t = 270000:  -0.045916834950447086\n",
      "State:  [0.64757371 1.33534273]\n",
      "Average reward at t = 280000:  -0.05467779016494751\n",
      "State:  [1.72264266 1.22262142]\n",
      "Average reward at t = 290000:  -0.061004836797714235\n",
      "State:  [0.7668817  1.27424628]\n",
      "Average reward at t = 300000:  -0.07509297370910645\n",
      "State:  [0.15437031 1.2710349 ]\n",
      "Average reward at t = 310000:  -0.05936590611934662\n",
      "State:  [0.25499487 1.23786148]\n",
      "Average reward at t = 320000:  -0.08230062294006348\n",
      "State:  [0.97992998 1.25791893]\n",
      "Average reward at t = 330000:  -0.048826103210449216\n",
      "State:  [0.10958099 1.27602519]\n",
      "Average reward at t = 340000:  -0.04196034300327301\n",
      "State:  [2.98676205 1.21680182]\n",
      "Average reward at t = 350000:  -0.07502866578102112\n",
      "State:  [0.22457278 1.27865965]\n",
      "Average reward at t = 360000:  -0.06502769052982331\n",
      "State:  [0.09709072 1.17471752]\n",
      "Average reward at t = 370000:  -0.06230869472026825\n",
      "State:  [1.71601796 1.46509508]\n",
      "Average reward at t = 380000:  -0.038140266299247744\n",
      "State:  [1.83653975 1.35387392]\n",
      "Average reward at t = 390000:  -0.023090000867843626\n",
      "State:  [0.28009868 1.35408689]\n",
      "Average reward at t = 400000:  -0.038815937757492064\n",
      "State:  [0.43807697 1.2102347 ]\n",
      "Average reward at t = 410000:  -0.08142971634864807\n",
      "State:  [2.08279037 1.25360878]\n",
      "Average reward at t = 420000:  -0.06463979041576386\n",
      "State:  [2.28665614 1.37637306]\n",
      "Average reward at t = 430000:  -0.06493249547481537\n",
      "State:  [0.68571764 1.48558722]\n",
      "Average reward at t = 440000:  -0.03959365701675415\n",
      "State:  [2.79136968 1.24857513]\n",
      "Average reward at t = 450000:  -0.05075377464294434\n",
      "State:  [1.09213388 1.44104804]\n",
      "Average reward at t = 460000:  -0.06954510319232941\n",
      "State:  [0.15538287 1.38036778]\n",
      "Average reward at t = 470000:  -0.04278515923023224\n",
      "State:  [0.95821106 1.19295596]\n",
      "Average reward at t = 480000:  -0.02938152062892914\n",
      "State:  [0.27700549 1.25236903]\n",
      "Average reward at t = 490000:  -0.08253941750526428\n",
      "State:  [5.20589066 1.29802906]\n"
     ]
    }
   ],
   "source": [
    "solve(actor_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
